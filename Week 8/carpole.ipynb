{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1   # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 2000\n",
    "num_bins = (16, 12, 16, 12)  # Refined discretization bins for state space\n",
    "\n",
    "# Function to discretize the continuous state space\n",
    "def discretize_state(state, num_bins):\n",
    "    state_array = state if isinstance(state, np.ndarray) else state[0]\n",
    "    \n",
    "    cart_pos_bins = np.linspace(-2.4, 2.4, num_bins[0])\n",
    "    cart_vel_bins = np.linspace(-3.0, 3.0, num_bins[1])\n",
    "    pole_angle_bins = np.linspace(-0.2, 0.2, num_bins[2])\n",
    "    pole_vel_bins = np.linspace(-3.0, 3.0, num_bins[3])\n",
    "\n",
    "    cart_pos_bin = np.digitize(state_array[0], cart_pos_bins) - 1\n",
    "    cart_vel_bin = np.digitize(state_array[1], cart_vel_bins) - 1\n",
    "    pole_angle_bin = np.digitize(state_array[2], pole_angle_bins) - 1\n",
    "    pole_vel_bin = np.digitize(state_array[3], pole_vel_bins) - 1\n",
    "\n",
    "    return (cart_pos_bin, cart_vel_bin, pole_angle_bin, pole_vel_bin)\n",
    "\n",
    "# Initialize Q-table (value function)\n",
    "num_actions = env.action_space.n\n",
    "num_states = (num_bins[0] + 1, num_bins[1] + 1, num_bins[2] + 1, num_bins[3] + 1, num_actions)\n",
    "Q = np.random.uniform(low=-1, high=1, size=num_states)\n",
    "# Q = np.zeros(num_bins + (num_actions,))\n",
    "\n",
    "# Function to choose action based on epsilon-greedy policy\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Explore action space\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit learned values\n",
    "\n",
    "# Function to update Q-table based on TD(0)\n",
    "def update_Q(state, action, reward, next_state, alpha, gamma):\n",
    "    current_estimate = Q[state][action]\n",
    "    best_future_estimate = np.max(Q[next_state])\n",
    "    td_target = reward + gamma * best_future_estimate\n",
    "    td_error = td_target - current_estimate\n",
    "    Q[state][action] += alpha * td_error\n",
    "\n",
    "# Initialize variables for training\n",
    "total_rewards = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state, num_bins)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state, num_bins)\n",
    "        episode_reward += reward\n",
    "\n",
    "        update_Q(state, action, reward, next_state, alpha, gamma)\n",
    "        state = next_state\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "    # Decay epsilon for exploration-exploitation trade-off\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Plot total rewards per episode\n",
    "plt.figure()\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.show()\n",
    "\n",
    "# Check if the agent has learned to balance the pole\n",
    "if max(total_rewards) >= 200:\n",
    "    print(\"The agent has learned to balance the pole.\")\n",
    "else:\n",
    "    print(\"The agent has not yet learned to balance the pole.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
