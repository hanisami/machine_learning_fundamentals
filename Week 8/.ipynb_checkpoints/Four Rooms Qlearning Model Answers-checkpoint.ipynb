{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8575fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /Users/hanisami/opt/anaconda3/lib/python3.9/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy in /Users/hanisami/opt/anaconda3/lib/python3.9/site-packages (from imageio) (1.24.4)\n",
      "Requirement already satisfied: pillow in /Users/hanisami/opt/anaconda3/lib/python3.9/site-packages (from imageio) (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd5702",
   "metadata": {},
   "source": [
    "## Importing The Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b11b0",
   "metadata": {},
   "source": [
    "## Creating The Four Rooms Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b13a063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FourRoomsEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(FourRoomsEnv, self).__init__()\n",
    "        self.grid_size = 11\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions: up, down, left, right\n",
    "        self.observation_space = spaces.Discrete(self.grid_size * self.grid_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        # Define borders and inner walls\n",
    "        self.grid[:, 0] = -1  # Left border\n",
    "        self.grid[:, -1] = -1  # Right border\n",
    "        self.grid[0, :] = -1  # Top border\n",
    "        self.grid[-1, :] = -1  # Bottom border\n",
    "        \n",
    "        # Inner walls\n",
    "        self.grid[5, 0:5] = -1\n",
    "        self.grid[5, 6:] = -1\n",
    "        self.grid[0:5, 5] = -1\n",
    "        self.grid[6:, 5] = -1\n",
    "        self.grid[5, 5] = -1\n",
    "        \n",
    "        # Open cells in the middle of each wall\n",
    "        self.grid[5, 2] = 0  # Middle of the left wall\n",
    "        self.grid[5, 8] = 0  # Middle of the right wall\n",
    "        self.grid[2, 5] = 0  # Middle of the top wall\n",
    "        self.grid[8, 5] = 0  # Middle of the bottom wall\n",
    "\n",
    "        # Define start and goal states\n",
    "        self.start_state = (1, 1)\n",
    "        self.goal_state = (9, 9)\n",
    "        self.state = self.start_state\n",
    "        return self.state_to_index(self.state)\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "\n",
    "    def index_to_state(self, index):\n",
    "        row = index // self.grid_size\n",
    "        col = index % self.grid_size\n",
    "        return (row, col)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = list(self.state)\n",
    "        if action == 0 and next_state[0] > 0:  # Up\n",
    "            next_state[0] -= 1\n",
    "        elif action == 1 and next_state[0] < self.grid_size - 1:  # Down\n",
    "            next_state[0] += 1\n",
    "        elif action == 2 and next_state[1] > 0:  # Left\n",
    "            next_state[1] -= 1\n",
    "        elif action == 3 and next_state[1] < self.grid_size - 1:  # Right\n",
    "            next_state[1] += 1\n",
    "\n",
    "        # Check if next state hits a wall (outer borders or inner walls)\n",
    "        if self.grid[next_state[0], next_state[1]] == -1:\n",
    "            reward = -5  # Penalty for hitting a wall\n",
    "            next_state = self.state  # Stay in current state\n",
    "        else:\n",
    "            reward = -1  # Default reward for moving\n",
    "\n",
    "        if next_state == list(self.goal_state):\n",
    "            reward = 10  # Reward for reaching the goal\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.state = tuple(next_state)\n",
    "        return self.state_to_index(self.state), reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51edbd",
   "metadata": {},
   "source": [
    "## Setting The Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Q-learning\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_min = 0.1  # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate for exploration probability\n",
    "num_episodes = 2000  # Increased episodes for better learning\n",
    "max_steps_per_episode = 100  # Reduced steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9b8c7",
   "metadata": {},
   "source": [
    "## Initializing the Environment, the Q-table, the epsilon-greedy policy, and the plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00d0e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = FourRoomsEnv()\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Explore action space\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Plot grid function\n",
    "def plot_grid(env):\n",
    "    agent_pos = env.index_to_state(env.state_to_index(env.state))\n",
    "    grid = np.copy(env.grid)\n",
    "    grid[agent_pos[0], agent_pos[1]] = 2  # Mark the agent's position\n",
    "    grid[env.goal_state[0], env.goal_state[1]] = 3  # Mark the goal state\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid, cmap=\"coolwarm\", origin=\"upper\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a351a4",
   "metadata": {},
   "source": [
    "## Training The Agent Using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with Q-learning\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        # Visualize the environment\n",
    "        plot_grid(env)\n",
    "#         plt.title(f\"Episode: {episode + 1}, Step: {step + 1}\")\n",
    "#         plt.show()\n",
    "\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "        td_error = td_target - Q[state, action]\n",
    "        Q[state, action] += alpha * td_error\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward  # Accumulate reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Decay epsilon\n",
    "\n",
    "    # Print progress every 5 episodes\n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Average Reward: {np.mean(episode_rewards[-5:]):.2f}\")\n",
    "\n",
    "# # Final visualization of learned policy (optional)\n",
    "# state = env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     clear_output(wait=True)\n",
    "#     plot_grid(env)\n",
    "#     plt.title(\"Final Policy Visualization\")\n",
    "#     plt.show()\n",
    "#     action = epsilon_greedy_policy(state)\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     state = next_state\n",
    "#     time.sleep(0.1)  # Adjust visualization speed as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
