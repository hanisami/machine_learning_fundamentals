{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34beea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa2182",
   "metadata": {},
   "source": [
    "## Importing The Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd116c",
   "metadata": {},
   "source": [
    "## Creating The Four Rooms Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourRoomsEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(FourRoomsEnv, self).__init__()\n",
    "        self.grid_size = 11\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions: up, down, left, right\n",
    "        self.observation_space = spaces.Discrete(self.grid_size * self.grid_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        # Define borders and inner walls\n",
    "        self.grid[:, 0] = -1  # Left border\n",
    "        self.grid[:, -1] = -1  # Right border\n",
    "        self.grid[0, :] = -1  # Top border\n",
    "        self.grid[-1, :] = -1  # Bottom border\n",
    "        \n",
    "        # Inner walls\n",
    "        self.grid[5, 0:5] = -1\n",
    "        self.grid[5, 6:] = -1\n",
    "        self.grid[0:5, 5] = -1\n",
    "        self.grid[6:, 5] = -1\n",
    "        self.grid[5, 5] = -1\n",
    "        \n",
    "        # Open cells in the middle of each wall\n",
    "        self.grid[5, 2] = 0  # Middle of the left wall\n",
    "        self.grid[5, 8] = 0  # Middle of the right wall\n",
    "        self.grid[2, 5] = 0  # Middle of the top wall\n",
    "        self.grid[8, 5] = 0  # Middle of the bottom wall\n",
    "\n",
    "        # Define start and goal states\n",
    "        self.start_state = (1, 1)\n",
    "        self.goal_state = (9, 9)\n",
    "        self.state = self.start_state\n",
    "        return self.state_to_index(self.state)\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "\n",
    "    def index_to_state(self, index):\n",
    "        row = index // self.grid_size\n",
    "        col = index % self.grid_size\n",
    "        return (row, col)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = list(self.state)\n",
    "        if action == 0 and next_state[0] > 0:  # Up\n",
    "            next_state[0] -= 1\n",
    "        elif action == 1 and next_state[0] < self.grid_size - 1:  # Down\n",
    "            next_state[0] += 1\n",
    "        elif action == 2 and next_state[1] > 0:  # Left\n",
    "            next_state[1] -= 1\n",
    "        elif action == 3 and next_state[1] < self.grid_size - 1:  # Right\n",
    "            next_state[1] += 1\n",
    "\n",
    "        # Check if next state hits a wall (outer borders or inner walls)\n",
    "        if self.grid[next_state[0], next_state[1]] == -1:\n",
    "            reward = -5  # Penalty for hitting a wall\n",
    "            next_state = self.state  # Stay in current state\n",
    "        else:\n",
    "            reward = -1  # Default reward for moving\n",
    "\n",
    "        if next_state == list(self.goal_state):\n",
    "            reward = 10  # Reward for reaching the goal\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.state = tuple(next_state)\n",
    "        return self.state_to_index(self.state), reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6912b1d3",
   "metadata": {},
   "source": [
    "## Setting The Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Q-learning\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_min = 0.1  # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate for exploration probability\n",
    "num_episodes = 2000  # Increased episodes for better learning\n",
    "max_steps_per_episode = 100  # Reduced steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f0fec",
   "metadata": {},
   "source": [
    "## Initializing the Environment, the Q-table, the epsilon-greedy policy, and the plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab382a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = FourRoomsEnv()\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Explore action space\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Plot grid function\n",
    "def plot_grid(env):\n",
    "    agent_pos = env.index_to_state(env.state_to_index(env.state))\n",
    "    grid = np.copy(env.grid)\n",
    "    grid[agent_pos[0], agent_pos[1]] = 2  # Mark the agent's position\n",
    "    grid[env.goal_state[0], env.goal_state[1]] = 3  # Mark the goal state\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid, cmap=\"coolwarm\", origin=\"upper\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925863a",
   "metadata": {},
   "source": [
    "## Training The Agent Using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b429d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with Q-learning\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        # Visualize the environment\n",
    "        plot_grid(env)\n",
    "\n",
    "        # Call The Epsilon Greedy Policy to take an \"action\"\n",
    "        # <write your code here!!>\n",
    "        # Take a step in the environment. (hind: the expected return is the next_state, reward, done, and info)\n",
    "        # <write your code here!1>\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "        td_error = td_target - Q[state, action]\n",
    "        \n",
    "        # Update the Q-value of the current state,action by alpha times the td_error\n",
    "        #<write your code here!!>\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward  # Accumulate reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Decay epsilon\n",
    "\n",
    "    # Print progress every 5 episodes\n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Average Reward: {np.mean(episode_rewards[-5:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc766bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
