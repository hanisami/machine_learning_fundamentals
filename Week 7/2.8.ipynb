{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbcfe94858f0b7b36435035aa68f1a494dacb16a"
   },
   "source": [
    "**Fashion MNIST - A Multilayer Perceptron**\n",
    "\n",
    "In this practical exercise, we'll make a simple neural network that gets ~90% accuracy on the Fashion MNIST dataset (a ten class, 28x28 image classification problem).\n",
    "\n",
    "For details about the dataset, check the following link:\n",
    "**Resources:**\n",
    "[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
    "\n",
    "\n",
    "The library we are going to use is Keras to implement the MLP. Check the following link for documentation:\n",
    "[Keras](https://keras.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5ede22b98539638a27b3c165fcaa5f28a6f3433"
   },
   "source": [
    "First up: importing modules. This model just feeds forwards, so we can use a `Sequential` class. As for the layers themselves, we're only using `Dense` and `Activation`. Nothing fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cda24f993b3fce92310d81fb3f6a782ed800778d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Prepartion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84b8c27283e746951f5d9a674187906076e964c1"
   },
   "source": [
    "Next some constants. `INPUT_SHAPE` is 784 (28 x 28 - flattened form of the image), and `NUM_CATEGORIES` is 10. All fairly self explanatory.  At the bottom, we use `pd.read_csv` to pull in our data, and we grab the `values` property, which is a numpy array version of the `DataFrame` we just read in.\n",
    "\n",
    "\n",
    "#### Note: Since the files of the data are large, you need to download and unzip it locally from: https://www.kaggle.com/datasets/zalando-research/fashionmnist/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2da26fa2385c17b1dab60db9deb52dc909ec612c"
   },
   "outputs": [],
   "source": [
    "#  DEFINE CONSTANTS\n",
    "\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CATEGORIES = 10\n",
    "\n",
    "LABEL_DICT = {\n",
    " 0: \"T-shirt/top\",\n",
    " 1: \"Trouser\",\n",
    " 2: \"Pullover\",\n",
    " 3: \"Dress\",\n",
    " 4: \"Coat\",\n",
    " 5: \"Sandal\",\n",
    " 6: \"Shirt\",\n",
    " 7: \"Sneaker\",\n",
    " 8: \"Bag\",\n",
    " 9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "# LOAD THE RAW DATA\n",
    "train_raw = pd.read_csv('./data/mnist_fashion/fashion-mnist_train.csv').values\n",
    "test_raw = pd.read_csv('./data/mnist_fashion/fashion-mnist_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1a009ca80dc5d7b90be13a17885c584903e548e"
   },
   "source": [
    "Next, we split the import data into training and testing data (as well as X and Y). Any \"x\" variable is an input, while \"y\" is the expected output. We set train and test x to everything but the first column of data in our input data (hence the slice), and use Keras' `to_categorical` to one-hot encode the output label to a vector of length `NUM_CATEGORIES` (10). We then normalize the X data. We change the range from 0 - 255 to 0 - 1 by dividing by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81fbbdc685be600913afddf6450c2e56dfd45a93"
   },
   "outputs": [],
   "source": [
    "# split into X and Y, after one-hot encoding\n",
    "train_x, train_y = (train_raw[:,1:], to_categorical(train_raw[:,0], num_classes = NUM_CATEGORIES))\n",
    "test_x, test_y = (test_raw[:,1:], to_categorical(test_raw[:,0], num_classes = NUM_CATEGORIES))\n",
    "\n",
    "# normalize the x data\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75bba2a8b4a7576acc0da50fc9e18863fed312b3"
   },
   "source": [
    "Now for the fun part - defining our model! In this case it's a simple four layer network - an input shape of `INPUT_SHAPE` (784), three 512 neuron layers, and an output layer with `NUM_CATEGORIES` neurons (10). We use categorical crossentroy as our loss, as we've got a multi-class classification problem. For an activation function, we use ReLU all the way, except for the output layer, which uses softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb4b6a269093a5658be8953446f2b9c013182b6a"
   },
   "outputs": [],
   "source": [
    "# BUILD THE MODEL\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer to the model with 512 neurons\n",
    "# Add a relu activation\n",
    "\n",
    "# <write your code here>\n",
    "# <write your code here>\n",
    "\n",
    "# Repeat the same process two more times: \n",
    "# Dense of 512 followed by relu activation + \n",
    "# Dense of 512 followed by relu activation +\n",
    "# Dense of 512 followed by relu activation\n",
    "\n",
    "# <write your code here>\n",
    "# ...\n",
    "\n",
    "model.add(Dense(NUM_CATEGORIES))\n",
    "#In the last layer, we add NUM_CATEGORIE as the output size. We still need to add a softmax activation!\n",
    "# <write your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the model - categorical crossentropy is for multiple choice classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model. Use rmsprop as optimizer, categorical_crossentropy as loss, and accuracy as metrics\n",
    "# You can check the Keras documentation for samples\n",
    "\n",
    "# <write your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6abf090dbb6df400f3d2cdf6cd180deb82fe4442"
   },
   "source": [
    "Finally, the training. We tell it to use our `train_x` and `train_y` as our training data, `test_x` and `test_y` to validate, use 32 samples per training pass, and run through the whole dataset 8 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fdb586c5dd6aea8d00bba48cd3762c97ecda9c2"
   },
   "outputs": [],
   "source": [
    "# train the model!\n",
    "model.fit(train_x,\n",
    "          train_y,\n",
    "          epochs = 8,\n",
    "          batch_size = 32,\n",
    "          validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbadcbdb313bb71047c6fb5e1d3015a7c5c1ce7a"
   },
   "outputs": [],
   "source": [
    "# how'd the model do?\n",
    "# Perform validation on the model for the train and test data\n",
    "\n",
    "# <write your code here>\n",
    "# <write your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe783c536d6e33d63d88a27afacffe870a3316e7"
   },
   "source": [
    "Nice! The first parameter is loss, while the second parameter is accuracy. almost 90%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
